---
title: "BD&DS: LGT"
date: "minor.bonilla@ulead.ac.cr"
author: "Minor Bonilla Gómez"
output:
  rmdformats::material:
    highlight: kate
    self_contained: true
    code_folding: show
    thumbnails: true
    gallery: true
    fig_width: 4
    fig_height: 4
    df_print: kable
---

```{r IMGLoop, echo=FALSE, out.width = "100px",fig.align="left", message=FALSE, warning=FALSE}
    IMG="https://media.giphy.com/media/Jad5PoGXGeZfq/giphy.gif"
    knitr::include_graphics(IMG)
```

# LIBRERIAS

Como buena práctica, siempre como primer paso llamar las librerías que serán utilizadas a lo largo del proceso de ejecución, haciendo con ello más fácil la lectura del documento. Cada vez que una librería sea invocada lo haremos a través de la orden **{NombreLibreria}::{NombreFuncion}**. Recuerde que en caso de no tener instalada la libreria necesaria, basta con llamar el comando **install.packages**.

```{r librerias, include=TRUE, message=FALSE, warning=FALSE}

library(devtools)
library(datarium)
library(stargazer)
library(ggfortify)
library(magrittr)
library(DT)
library(ElemStatLearn)
library(caret)
library(rlang)
library(regclass)
```


# REGRESION LOGISTICA {.tabset .tabset-fade .tabset-pills}

```{r, include = FALSE}
knitr::opts_chunk$set(cache = TRUE, autodep = TRUE, fig.align = "center")
```

> ***"...Good teachers teach things, great teachers teach how to learn and how to think..."
> -Kevin Zeng Hu-***

## OLS a LOGIT

Ahora nuestro fenómeno de estudio se ha sido reducido a dos posibles estados $0$ y $1$.

Aunque puede parecer extraño a primera vista para algunos -o una sobre simplificación para otros- estos $0$s y  $1$s son capaces de resumir de manera natural muchas condiciones cotidinas del tipo si/no, bueno/malo, saludable/enfermo, etc.

$$
Y = 
\begin{cases} 
      1: & \text{si} \\
      \\
      0: & \text{no} 
\end{cases}
$$

## INTUICION

Podemos entonces enfocarnos en definir una métrica que nos permita diferenciar a uno de otro estado, pero además condicionando éste por un conjunto de información que nos permita separar a unos de otros. 

Para ello apelaramos a la nocion de probabilidad, es decir nos interesa poder asignar probabilidades de pertenencia a uno u otro estado, dado un conjunto de información X

$$
  p(x) = P~[~Y = 1~|~X = x~]
$$

Por completitud, sabemos que habiendo dos estados, la probabilidad del SI adicionada a la probabilidad del NO suman el 100%, es decir, lo único cierto es que cada individuo pertenece a uno u otro estado (similar al tradicional lanzamiento de moneda que resulta en CARA|CRUZ, o experimento de Bernoulli).

$$
  P~[Y=0|X=x]~+P~[Y=1|X=x]~=~1
$$

Por conveniencia, fijaremos nuestra atención al estado (1: SI) pues siendo dos estados, conociendo el primero, el segundo queda automáticamente definido; si un evento tiene 75% probabilidad de ocurrir, reciprocamente tiene 25% de no hacerlo.

con lo que el estado $Y = 0$ puede ser obtenido de manera trivial al conocer la probabilidad de ocurrencia del estado $Y = 1$

$$
  P~[Y=0~|~X=x] = 1 - p(x)
$$
Pero y que tal si antes de ignorar el estado $Y = 0$ le damos un uso adicional? 

Dado que tenemos dos estados y con ellos las probabilidades asociadas, porqué no tomar una razón que nos permita validar qué tanto más probable es un estado que el otro.

Bastaria con dividir las probabilidades de uno y otro estado: 

$$\frac{P_{~(Y=1)}}{P_{~(Y=0)}}$$

sabiendo de partida que si dicha razón arroja un valor igual a $1$ si ambos eventos tienen la misma probabilidad; 

si la razón es mayor a $1$ tenemos que la probabilidad de que se alcance el estado *1* es mas probable a alcanzar el estado *0*, reciprocamente cuando la razon total es menor a 1.

Hasta ahora, parece que nuestra idea funciona, sólo que tenemos el inconveniente de que para eventos en que una de las probabilidades es muy pequeña con respecto a la otra nuestro operador puede dispararse al infinito. Por tanto interesa entonces restringir ese posible comportamiento a un intervalo; Para ello basta con aplicar el logaritmo a dicha razón.

Felicidades, hemos llegado de manera intuitiva al concepto de **log odds**

## NOTACION

Usemos ahora nuestra notación habitual, para aquellos que la prefieren a la narrativa. Tomamos la relación *OCURRE / NO OCURRE* y la espresamos como

$$
  \frac{p(x)}{1 - p(x)} = \frac{P[Y = 1 |  X = x]}{P[Y = 0 | X = x]}
$$
Dado que nuestro operador está ahora restringido al intervalo ]0,1[ qué tal si nos regresamos a nuestro modelo de regresión lineal y tratamos ahora de predecir nuestro operador **log odd** a través de una **Combinación Lineal** de nuestras $X_p$ variables descriptoras del fenómeno.


$$
\log\left(\frac{p(x)}{1-p(x)}\right) = \beta_0 + \beta_1 x_1 + \ldots  + \beta_{p} x_{p}
$$

Basicamente "nuestra" propuesta ya existe :( y es conocida como funcion LOGIT

[logit](https://en.wikipedia.org/wiki/Logit)


$$
\text{logit}(\xi) = \log\left(\frac{\xi}{1 - \xi}\right)
$$

Y si tomamos la funcion inversa de la función *logit*? 

Esta forma inversa de nuestra *LOGIT* es comocida como función *LOGISTICA* y se torna de gran utilidad en nuestro problema para descifrar el vector de coeficientes $\beta$ (alguna literatura trata a la funcion *LOGISTICA* como funcion *SIGMOIDE*)

[función sigmoide](https://en.wikipedia.org/wiki/Sigmoid_function){target="_blank"}

$$
\text{logit}^{-1}(\xi) = \frac{e^\xi}{1 + e^{\xi}} = \frac{1}{1 + e^{-\xi}}
$$

notar que esta función define a $x \in\ (-\infty, \infty))$, retornando valores $y \in\ ]0, 1[$. 

La conveniencia de haber tomado la inversa nos lleva a una forma reducida que será nuestro objeto de estimación en adelante para obtener nuestro vector \beta.

$$
p({x}) = P[Y = 1 | X =  x] = \frac{e^{\beta_0 + \beta_1 x_{1} + \cdots + \beta_{p-1} x_{(p-1)}}}{1 + e^{\beta_0 + \beta_1 x_{1} + \cdots + \beta_{p-1} x_{(p-1)}}}
$$

$$
p(x) = P[Y = 1 |X = x] = \frac{1}{1 + e^{-\beta_0 - \beta_1 x_{1} - \cdots - \beta_{p-1} x_{(p-1)}}}
$$

# OBTENCION de COEFICIENTES

Ahora nuestra tarea se redujo a obtener los parámetros $\beta$

$$
\boldsymbol{{\beta}} = [~\beta_0,~ \beta_1,~ \beta_2,~ \beta_3,~ \ldots,~ \beta_{p}~]
$$

para ello emplearemos el estimador de Maxima Verosimilitud. 

Nos interesa conocer la probabilidad del conjunto de datos de manera simultánea y no de cada observación de manera individual. Para ello pensamos que nuestra colección de datos fue generado por un mismo proceso de manera independiente, haciendo que la probabilidad conjunta de dicha realización sea la probabilidad de haber ocurrido el evento 1, el evento 2, ..., el evento n-esimo de manera simultanea. De ahi que si los eventos son independientes, ésto su probabilidad vendrá dada por la multiplicatoria de todos los eventos de manera simultánea.

$$
L(\boldsymbol{{\beta}}) = \prod_{i = 1}^{n} P[Y_i = y_i | X_i = x_i]
$$

reexpresando,

$$
L(\boldsymbol{{\beta}}) = \prod_{i = 1}^{n} p({\bf x_i})^{y_i} (1 - p({\bf x_i})^{(1 - y_i)}
$$

$$
L(\boldsymbol{{\beta}}) = \prod_{i : y_i = 1}^{n} p(x_i) \prod_{j : y_j = 0}^{n} (1 - p({\bf x_j})
$$

$$
L(\boldsymbol{{\beta}}) = \prod_{i : y_i = 1}^{} \frac{e^{\beta_0 + \beta_1 x_{i1} + \cdots + \beta_{p-1} x_{i(p-1)}}}{1 + e^{\beta_0 + \beta_1 x_{i1} + \cdots + \beta_{p-1} x_{i(p-1)}}} \prod_{j : y_j = 0}^{} \frac{1}{1 + e^{\beta_0 + \beta_1 x_{j1} + \cdots + \beta_{p-1} x_{j(p-1)}}}
$$

A diferencia de Mínimos Cuadrados la obtención del vector $\beta$ no proviene de un resultado analítico (recuerde: En minimos cuadrados vimos que $\beta = {(x'x)}^{-1} x'y$) sino que será necesario un método numérico para obtener el vector de coeficientes (habitualmente el de Newton-Raphson). 

Si obviamos ese paso, podemos apoyarnos en `R` que tiene especificados una serie de algoritmos para resolver este tema particular.

# CONTRATACIÓN {.tabset .tabset-fade .tabset-pills}

## ENTREVISTA

Para someter a prueba a nuestros analistas, hemos decidido crear un conjunto de datos artificiales sobre los que tenemos pleno control. Sabemos que Jordan Cristiano Navas [A1] tiende a sobre dimensionar sus conocimientos y pensamos que está siendo victima de las modas. Por otro lado Bryan Messi Chinchilla [A2] muestra un perfil completamente distinto que tambien queremos evaluar.

Dimos a ambos el mismo conjunto de datos. *A1* de manera inmediata respondió con la herramienta que más conoce (o de la que más ha oído hablar en sus foros de 'Data Scientists': Regresión lineal)

Veamos qué sucede con sus resultados, a partir de que nosotros sabemos que los datos tienen las siguientes características:

$$
\log\left(\frac{p({\bf x})}{1 - p({\bf x})}\right) = -2 + 3 x
$$

que como sabemos de nuestra lectura puede ser reexpresado como:

$$
\begin{aligned}
Y_i \mid {\bf X_i} = {\bf x_i} &\sim \text{Bern}(p_i) \\
p_i &= p({\bf x_i}) = \frac{1}{1 + e^{-\eta({\bf x_i})}} \\
\eta({\bf x_i}) &= -2 + 3 x_i
\end{aligned}
$$

algo que desconoce nuestro *A1* dado que eso "no le resultó importante" dada su limitacion... De tiempo.

## SET DE DATOS

Así las cosas, procedemos a generar nuestro conjunto de datos con las siguientes instrucciones

```{r, echo=TRUE}
set.seed(123456789)
nDatos = 25 
beta_0 = -2
beta_1 = 3

x = rnorm(n = nDatos)
phi = beta_0 + beta_1 * x
p = 1 / (1 + exp(-phi))
y = rbinom(n = nDatos, size = 1, prob = p)
DatosLogisticos=data.frame(y, x)
head(DatosLogisticos)
```

La simulación del conjunto de datos pretende de manera controlada realizar el contraste entre instrumentos y con ello evidenciar el problema de emplear un martillo para cortar una madera fina.

## RESULTADOS

Tengamos presente que nuestra variable *Y* toma sólo dos valores, a saber, 0 ó 1, algo que parece no incomodar a nuestro *A1* quien decide usar el único instrumento que conoce para hacer frente al problema de estimación que le ha sido asignado. 

Paralelamente *A2* se percata de la estructura de los datos, los estudia y decide estimar un modelo más adecuado para variable binaria, dado que observa que la variable *Y* presenta este comportamiento.

```{r,, echo=TRUE}
# Analista 1. Estimación por Minimos Cuadrados Ordinarios
fit_An1  = glm(y ~ x, data = DatosLogisticos, family = gaussian)
# Analista 1. Estimación Logistica
fit_An2 = glm(y ~ x, data = DatosLogisticos, family = binomial)
```

Nuestro *A1* ha decidido no prestar importancia al parametro [family] pues eso "no lo entiende" pero "parece lo mismo" que encontró en una web de la que simplemente "copió y pegó" lo que pudo.

Adicionalmente nuestro *A2* -quien sabemos hace menor alarde de su conocimiento- sabe que cuando llega el momento de realizar predicciones, el objeto 'glm' -que está preparado para retornar valores distintos en función a lo que se le solicita- le retornará algo que podria no necesitar. 

*R* ha dispuesto el parametro 'type' que a solicitud retorna, nuestra conocida razon *odds* o la probabilidad calculada para cada observación

## EVALUANDOLOS                    

```{r, , echo=TRUE}
plot(y ~ x, data = DatosLogisticos, 
pch = 20, ylab = "Probabilidad Estimada", 
main = "Minimos Cuadrados [MCO] vs Regresion Logistica [RLG]")
grid()
abline(fit_An1, col = "darkorange")
curve(predict(fit_An2, data.frame(x), type = "response"), add = TRUE, col = "dodgerblue", lty = 2)
legend("topleft", c("MCO", "RLG", "Data"), lty = c(1, 2, 0), 
pch = c(NA, NA, 20), lwd = 2, col = c("darkorange", "dodgerblue", "black"))
```

de manera conveniente la variable *Y* fue creada a partir de un único regresor, lo que nos facilita su visualización. 

Recordemos que

$$
\log\left(\frac{p(x)}{1 - p(x)}\right) = -2 + 3 x
$$

En naranja se muestra la estimación realizada empleando Regresion Lineal, mientras que la linea punteada contiene la estimación obtenida a partir de la regresión logística.

$$P[Y = 1 | X = {\bf x}]$$

Sabiendo que estamos interesados en obtener un estimador para la probabilidad de cada uno de los individuos, notamos de manera inmediata un problema con la regresion lineal: Lo ven?

Como esperábamos las probabilidades obtenidas empleando la regresión logística se encuentran en el intervalo $]0,1[$ 

Le pedimos ahora a nuestros analistas, que realicen algunas predicciones con sus modelos para algunos valores específicos de *X*

```{r, , echo=TRUE}
library(kableExtra)
x_test=c(-6.0,-2,0,2,6.0)
kable(x_test)
```

para ello cada quien necesitará recuperar los coeficientes asociados a su estimación.

* ANALISTA1:
```{r, , echo=TRUE}
coef(fit_An1)
```

Con lo que el modelo para el *A1* toma la forma:

$$
Y = 0.29 + 0.25 X
$$
* ANALISTA2:
```{r, , echo=TRUE}
#ANALISTA2
coef(fit_An2)
```

mientras que el modelo de *A2*:

$$
\hat{P}[Y = 1 | X ] = \frac{e^{-2.1 + 2.7 X}}{1 + e^{-2.3 + 2.7 X}}
$$

veamos que pasó con las probabilidades de uno y otro:

* ANALISTA1:
```{r}
prob1=predict(fit_An1, data.frame(x = c(-6.0,-2,0,2,6.0)))
round(prob1,2)
```

* ANALISTA2:
```{r, , echo=TRUE}
prob2=predict(fit_An2, data.frame(x = c(-6.0,-2,0,2,6.0)), type="response")
round(prob2,2)
```

Los resultados obtenidos por *A1* parecen ser un problema. Sus probabilidades no sólo arrojan probabilidades superiores al 100% sino que además tiene probabilidades negativas! 

# DIAGNOSTICOS

Como lo habiamos hecho con la regresion lineal, debemos ahora evaluar si los resultados tienen validez estadística. A modo de definir un *proceso* o *lista de chequeo* éste tipo de elementos son aquellos que con mayor frecuencia prestaremos atencion (esta lista no es exhaustiva ni pretende ser un recetario)

## Significancia individual de los parámetros $\beta$

### Test de Wald

En regresión lineal nos planteabamos la hipotesis de que nuestros coeficientes fueran iguales a *CERO*

$$
H_0: \beta_j = 0 \quad \text{vs} \quad H_1: \beta_j \neq 0
$$

En regresion logistica tambien podemos plantearnos dicha hipotesis, asi que de manera general podemos preguntarle a TODOS los coeficientes si son ellos -de manera simultanea- iguales a cero (lo que nos devolveria a un modelo de probabilidad incondicional, por que?) 

Nos basta un pequeño ajuste para poder reutilizar el conocido test de Wald de Regresion Lineal o test $t$. Cuando nos movemos a la regresion logística éste test deja de distribuir como $T$ y en su lugar distribuye como $NORMAL$ tomando una forma similar a Regresion Lineal, con el unico cambio mencionado en su distribución. 

$$
  z~~ = ~~\frac{\hat{\beta}_j - \beta_j}{\text{SE}[\hat{\beta}_j]} 
      ~~\overset{\text{aprox}}{\sim}
      ~~N(0, 1)
$$

#### Razón de Verosimilitud

$$
H_0: \beta_1 = \beta_{2} = \cdots = \beta_{p} = 0.
$$
Recordemos que para obtener los coeficientes del modelo logistico, empleamos el concepto de Maxima Verosimilitud, que no es mas que una función que retorna un valor. Denoemos como *L* a la funcion de Maxima Verosimilitud y procedamos a comparar el resultado que nos retorna dicha función con nuestro modelo estimado versus el modelo en que todos nuestros coeficientes resultan iguales a cero. Es decir, vamos a contrastar si la incorporación de las variables *X* o información, mejoró nuestra estimación con respecto a uno que otorga la misma probabilidad a todas las observaciones. 

Se define el test de Wald como la razon logaritmica entre en modelo NULO versus uno COMPLETO. Denotando dicho contraste como $D$ tenemos que:

$$
D = -2 \log \left( \frac{L(\boldsymbol{\hat{\beta}_{\text{Nulo}}})} {L(\boldsymbol{\hat{\beta}_{\text{Completo}}})} \right) = 2 \log \left( \frac{L(\boldsymbol{\hat{\beta}_{\text{Completo}}})} {L(\boldsymbol{\hat{\beta}_{\text{Nulo}}})} \right) = 2 \left( \ell(\hat{\beta}_{\text{Completo}}) - \ell(\hat{\beta}_{\text{Nulo}})\right)
$$

Sabemos que cuando la muestra es grande, este test distribuye como una Chi-Cuadrado 

$$
   D \overset{\text{aprox}}{\sim} \chi^2_{k}
$$

con $k = p - q$ denotando la diferencia entre la cantidad de coeficientes de uno y otro modelo. A este test le conocemos como *Razon de Verosimilitud*


# `SAheart`

Los datos provienen de una muestra de masculinos con problemas cardiacos en una zona de alta incidencia en Sudafrica. 

```
chd:       Indicates whether or not coronary heart disease is present in an individual
sbp:       Systolic blood pressure
tobacco:   Cumulative tobacco (kg)
ldl:       Low density lipoprotein cholesterol
adiposity: A numeric vector
famhist:   Family history of heart disease, a factor with levels Absent Present
typea:     Type-A behavior
obesity:   A numeric vector
alcohol:   Current alcohol consumption
age:       Age at onset
coronary:  Heart disease
```


```{r}
data("SAheart")
```

```{r, echo = FALSE}
knitr::kable(head(SAheart))
```

Iniciamos estimando el modelo de probabilidad asociado al problema cardiaco

$$
\log\left(\frac{P[\texttt{CHD} = 1]}{1 - P[\texttt{CHD} = 1]}\right) = \beta_0 + \beta_{\texttt{LDL}}  X_{\texttt{LDL}}
$$

```{r, echo=TRUE}
chd_mod_ldl = glm(chd ~ ldl, data = SAheart, family = binomial)
plot(jitter(chd, factor = 0.1) ~ ldl, data = SAheart, pch = 20, 
     ylab = "Probability of CHD", xlab = "Low Density Lipoprotein Cholesterol")
grid()
curve(predict(chd_mod_ldl, data.frame(ldl = x), type = "response"), 
      add = TRUE, col = "dodgerblue", lty = 2)
```

Contrastamos las probabilidades obtenidas con los valores observados. Se puede notar como al incrementar *LDL* incrementa la probabilidad del estado *CHD*


```{r, echo=TRUE}
coef(summary(chd_mod_ldl))
```

Nos interesa validar la hipótesis sobre la relevancia individual de nuestras variables, retomando el test de **WALD** 

$$
H_0: \beta_{\texttt{LDL}}  = 0
$$

Así como lo hicimos con Regresión Lineal, debemos estar atentos al valor  "$t$" (en realidad $z$) que retorna *R* cuando solicitamos hacemos uso de la función `summary()` y propiamente su probabilidad asociada. Ese bajo valor, nos permite rechazar la hipótesis nula de que el coeficiente asociado al variable *LDL* sea igual a cero, sugiriendo que dicha variable es un buen predictor de nuestro fenomeno en estudio.


pero veamos que pasa si agregamos el resto de variables:

```{r, echo=TRUE}
chd_mod_additive = glm(chd ~ ., data = SAheart, family = binomial)
```

teniendo ahora el conjunto de variables completo, podemos plantearnos la 2da hipotesis: Son conjuntamente todos los coeficientes iguales a cero? 

$$
H_0: \beta_{\texttt{sbp}} = \beta_{\texttt{tobacco}} = \beta_{\texttt{adiposity}} = \beta_{\texttt{famhist}} = \beta_{\texttt{typea}} = \beta_{\texttt{obesity}} = \beta_{\texttt{alcohol}} = \beta_{\texttt{age}} = 0
$$

que como recordamos no es mas que una diferencia de logaritmos que en *R* podemos plantear como:

```{r, echo=TRUE}
-2 * as.numeric(logLik(chd_mod_ldl) - logLik(chd_mod_additive))
```

o alternativamente podemos pedirle a R que lo haga por nosotros:

```{r, echo=TRUE}
anova(chd_mod_ldl, chd_mod_additive, test = "LRT")
```

El resultado del p-value es lo suficientemente bajo, como para  descartar la hipotesis de que nuestros coeficientes sean todos iguales a cero, con lo que comenzamos a pensar que nuestros predictores cumplen bien su tarea.

## Intervalos de Confianza

Recordemos que nuestros parámetros siguen su propia distribución, con lo que además de su valor medio (el que retornan los paquetes) nos interesa conocer el intervalo que circunda a nuestro parametro.

```{r, echo=TRUE}
chd_mod_additive = glm(chd ~ ., data = SAheart, family = binomial)
stargazer(chd_mod_additive, type='text')
```

Para ello *R* facilita la funcion `confint()` que realiza dicha tarea.

```{r, echo=TRUE}
confint(chd_mod_additive, level = 0.99)
```

# CLASIFICACION

Gran parte de la importancia de la metodologia de Regresión Logística consiste en poder obtener una metrica que nos permita separar nuestras observaciones. Esta métrica (que en este caso particular toma la forma de probabilidad) es uno de lo resultados mas valorados en la estimacion pues nos permite asignar a cada observacion al grupo $Y = 1$ o $Y = 0$



```{r, warning=FALSE, echo=TRUE}
datos=SAheart
datos$Y_pred=predict(chd_mod_additive,SAheart, type="response")
confusion_matrix(chd_mod_additive)
#check_regression(chd_mod_additive,seed=123)
```

<!---
```
Method 2 is a Hosmer-Lemeshow type goodness of fit test. The observations are put into 10 groups according to the probability predicted by the logistic regression model. For example, if there were 200 observations, the first group would have the cases with the 20 smallest predicted probabilities,
the second group would have the cases with the 20 next smallest probabilities, etc. The number of cases with the level of interest is compared with the expected number given the fitted logistic regression model via a chi-squared test. The test is failed is the p-value is less than 0.05.
```

-->