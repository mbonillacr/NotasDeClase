---
title: "Big Data & Decision Science: ACP"
date: "minor.bonilla@ulead.ac.cr"
author: "Minor Bonilla Gómez"
output:
  rmdformats::material:
    highlight: kate
    self_contained: true
    code_folding: show
    thumbnails: true
    gallery: true
    fig_width: 4
    fig_height: 4
    df_print: kable
---

```{r echo_f, include=FALSE, message=FALSE, warning=FALSE}
  knitr::opts_chunk$set(echo = TRUE)
  CLASE01="C:/Users/minor.bonilla/Pictures/PRESENTACIONES/CLASES/CL01/"
  CLASE00="C:/Users/minor.bonilla/Pictures/PRESENTACIONES/CLASES/CL00/"
```

# LIBRERIAS

```{r librerias, include=TRUE, message=FALSE, warning=FALSE}

# Dado que algunas de estas librerias podrían no estar instaladas en nuestra máquina, resulta de suma 
# importancia instalarlas antes -de ser necesario-; para ello usaremos el comando *install.packages* que 
# en nuestro caso particular: install.packages("NOMBRE_LIBRERIA")
  
  library('mvtnorm')
  library('ggplot2')
  library('GGally')

  library('magrittr')
  library('kableExtra')

  library('readxl')
  library('MASS')
  library('knitr')
  library('FactoMineR')
  library('factoextra')
  library("corrplot")

  library('plotly')
  library('xfun')

```

# INTRODUCCIÓN {.tabset .tabset-fade .tabset-pills}

Damos un salto en la dirección de las variables. Hasta ahora nos hemos enfocado en los individuos, pero daremos ahora cabida a las variables y a sus eventuales interacciones (y ambos!) 

Vale la pena hacerse entonces preguntas sobre los nuevos actores; son esas variables relevantes? Cuales? En qué direccion?

Nos abocaremos a comprender las relaciones entre las variables y a partir de esas relaciones, trataremos luego de dirigirnos en la direccion de la reducción de dimensiones. Para ello nos estudiaremos el Análisis de Componentes Principales (ACP). 
ACP es una técnica popularmente empleada en las etapas iniciales del proceso de modelación de datos, principalmente cuando se pretende obtener información de la varianza, misma que transformaremos a traves del propio proceso, maximizando la extracción de información de nuestros datos.

**PARSIMONIA** 

> En igualdad de condiciones, la explicación más sencilla suele ser la más probable. 
> Esto implica que, cuando dos teorías en igualdad de condiciones tienen las mismas 
> consecuencias, la teoría más simple tiene más probabilidades de ser correcta que la compleja.
> https://es.wikipedia.org/wiki/Navaja_de_Ockham


Así las cosas, queremos reducir (la cantidad de dimensiones) sin perder información relevante.

# VA's {.tabset}

* Momentos 1 y 2
* Media Condicional
* X ya no es determinística

Para esta seccion emplearemos el paquete `mvtnorm` que contiene múltiples herramientas para la generación de conjuntos de datos con distribuciones multinormales

Usaremos la función `rmvnorm` que requiere por parámetros:

1. Cantidad de observaciones a crear (*n*), 
2. Vector de promedios (tantas como variables a crear)
3. Matriz de covarianzas (pXp)

```{r, out.width = "800px"}
  require(mvtnorm)
  library(kableExtra)

# Vamos a generar dos variables, cada una con 100 observaciones
# si quieren pueden hacerlo BIG!

  require(mvtnorm)
  
  set.seed(123456789)

  N=100  
  PROMEDIOS=c(0, 2.5, -2)
  CORRELACION=matrix(c(1.5,-1.1,0,-1.1,2,0.5,0,0.5,3), nrow=3, ncol=3)
  Datos = mvtnorm::rmvnorm(n=N, mean=PROMEDIOS, sigma=CORRELACION)
  Datos = data.frame(Datos)
  Datos <- as.data.frame(Datos)
  colnames(Datos)=c("X","Y","Z")
  
  
  X=Datos[,1]
  Y=Datos[,2]
  Z=Datos[,3]
  
  dt=head(Datos, 10)
  
  kable(dt) %>%
  kable_styling(bootstrap_options = "striped", full_width = F, position ="center")#   "float_right")
  
  colMeans(Datos)
  
  var(Datos)
  
  plotly::plot_ly(Datos, x = ~X, y = ~Y, z= ~Z, alpha = 0.4)%>%
          add_markers(marker = list(line = list(color = "black", width = 0.25)))
  
```

Revisando los valores promedio de las series generadas

# VAR-COV {.tabset}

La covarianza indica el grado de variación conjunta de dos variables aleatorias respecto a sus medias, permitiendonos determinar si existe algun grado dependencia entre ambas variables. 

$$
s_{xy}=\sum_{i=1}^n{\frac{(x_{i}-\mu_{x})(y_{i}-\mu_{y})}{n}}
$$

$$
 \mathbf s_{xy}:
  \begin{cases}
    >0   & \quad \text{covarían en la misma dirección}\\
    =0   & \quad \text{no se observa evidencia de relación entre }(x,y)\\
    <0   & \quad \text{covarían en dirección opuesta}\\
  \end{cases}\
$$

Su importancia es de elevada relevancia en el análisis de datos pues es necesario para estimar otros parámetros básicos, como el coeficiente de correlación lineal u obtener los coeficientes asociados a la regresión.

$$\mathbf {s} = \left[\begin{array}
{rrr}
cov(x_1, x_1) & cov(x_1, x_2) & cov(x_1, x_3) & ... & cov(x_1, x_p) \\
cov(x_2, x_1) & cov(x_2, x_2) & cov(x_2, x_3) & ... & cov(x_2, x_p) \\
... & ... & ... &  ... & ...\\
cov(x_p, x_1) & cov(x_p, x_2) & cov(x_p, x_3) & ... & cov(x_p, x_p) \\
\end{array}\right]$$

notar que 

$$cov(x_1, x_1) = var(x_1)$$
por lo que:

$$\mathbf{s} = \left[\begin{array}
{rrr}
var(x_1) & cov(x_1, x_2) & cov(x_1, x_3) & ... & cov(x_1, x_p) \\
cov(x_2, x_1) & var(x_2) & cov(x_2, x_3) & ... & cov(x_2, x_p) \\
... & ... & ... &  ... & ...\\
cov(x_p, x_1) & cov(x_p, x_2) & cov(x_p, x_3) & ... & var(x_p) \\
\end{array}\right]$$


## Covarianza: Matrices {.tabset}

### Centrado, Escalado y Normalización

Comúnmente como primer paso en el proceso de ACP, resulta necesario centrar y normalizar los datos, dado que el análisis de ACP es sensible a las magnitudes de las variables. Esto toma importancia cuando nuestro set contiene datos medidos en diversas unidades. Así las cosas y dado que nuestro objetivo es explicar varianza, podríamos resultar siendo víctimas de asignar importancia a variables no por su aporte, sino por su unidad de medida.

Algunos métodos de escalamiento:

| Método de Escalamiento | Fórmula |
| :--------------------- | :-------------------------------------------------------------------------: |
| Centrado | $\large f(x) ~=~ x - \bar x$ |
| Escalamiento Unitario [0, 1] | $\large f(x) ~=~ \frac {x - min(x)} {max(x) - min(x)}$ |
| Escalamiento Intervalo [a, b] | $\large f(x) ~=~ (b - a)* \Large \frac {x - min(x)} {max(x) - min(x)} + a$ |
| Normalización | $\large f(x) ~=~ \frac {x - mean(x)} {\sigma_x}$ |

### Covarianza: 2 variables {-}

Para dos variables el calculo es bastante simple y puede realizarse "a pie" a partir de:

$$cov(x, y) ~=~ \frac{1}{N} \sum_{i=1}^N (x_i - \bar x) (y_i - \bar y)$$

### Covarianza >2 Variables {-}

Pero claramente enfrentamos en la realidad cotidiana casos de mayores a dos variables, para ello recurriremos a algunos elementos de cálculo matricial, que haran el proceso mucho más eficiente para nosotros.

Sea $\large X_{(n,p)}$ nuestra tabla de datos using linear algebra

1. Obtenga los valores promedio de cada variable $X_{promedio}$.
2. Compute la matriz de datos centrados $D ~=~ X - X_{promedio}$.
3. La matriz de covarianza viene dada por:

$$cov ~ (X) ~=~ \left( \frac{1}{N-1} \right) ~ D^T \cdot D $$
con 
$$ D = X - X_{Promedio}$$

siendo $D^T$ la matriz transpuesta de $D$

### Var-Cov: Ejemplo A PIE  {-}

```{r}
  Datos.escalado=scale(Datos, center=TRUE, scale=FALSE )
  n=nrow(Datos.escalado)
  print(n)
  Cov.aPata= t(Datos.escalado) %*% Datos.escalado
  
  1/(n-1)*Cov.aPata
  
  cov(Datos)
```

```{r, echo=FALSE}
    xfun::embed_file('G:/LEAD/2020/Essentials/CLASES/CL0003/TEMA002_ACP_DS.xlsx')
```

## Correlación
Ahora bien, dado que la matriz de covarianzas puede estar influenciada por el nivel de medición de una u otra variables, sus comparaciones no resultan inmediatas. De ahi, que requerimos un indicador que nos permita realizar comparaciones directas entre pares de variables. 

Nace la nocion de correlación, que no es más que una relativización de la matriz de Varianzas y Covarianzas. Basta entonces con realizar un ajuste y tenemos que:

$corr(x_1,x_2) = \frac{cov(x_1,x_2)}{\sigma_{x_1}\cdot\sigma{_{x_2}}}$

lo que nos permite tener una medida además restringida en el intervalo [-1, 1] de esta forma

$
  \[ Corr_{xy} :
    \begin{cases}
      \in \text{ } ]0,1[  & \quad \text{la relación entre variables es positiva} \\
      =0           & \quad \text{no se observa evidencia de relación entre }(x,y)\\
      \in \text{ } ]-1,0[ & \quad \text{las variables presentan relación inversa}\\
    \end{cases}
  \]
$


```{r}
  cov(Datos$X, Datos$Y) / (sd(Datos$X) * sd(Datos$Y))
  cov(Datos$X, Datos$Z) / (sd(Datos$X) * sd(Datos$Z))
  cov(Datos$Z, Datos$Y) / (sd(Datos$Z) * sd(Datos$Y))
```

no obstante *R* nos facilita una forma inmediata de calculo a partir del uso de la funcion *cor*
```{r}
  cor(Datos)
```

```{r, out.width = "800px"}
  pairs(Datos)
  GGally::ggpairs(Datos, )
```

# ANÁLISIS DE COMPONENTES PRINCIPALES (ACP) {.tabset}

Volviendo al principio de la parsimonia, queremos reducir la dimensionalidad. Para ello requerimos un proceso que permita objetivamente llevar a cabo esta tarea. El ACP es un proceso de transformación de los datos, en que nuevas variables (Componentes Principales *CP*) contienen información de las múltiples dimensiones, a partir de la combinación lineal de éstas.

Algunas propiedades de los CPs hacen que esta metodología sea deseable para procesos posteriores, entre ellas la ortogonalidad entre los pares de Componentes. Otra característica de gran relevancia, consiste en la forma en que es almacenada la información o varianza del conjunto de datos, pues ésta, es almacenada de manera natural en los primeros *P* componentes de nuestras *P* dimensiones, de forma tal que bastan *K* componentes para retener una cantidad importante de información de los datos, reduciendo en *P-k* dimensiones el espacio de las variables.

Así las cosas, cada CP puede ser interpretado geometricamente como los vectores dirección de los datos en sus *k* dimensiones, haciendo que la variabilidad del conjunto completo de datos sea proyectado en un espacio de menor dimensión (han visto televisión?)

## Intuición {.tabset}

Cada vector propio no es más que un conjunto de ponderadores de nuestras variables (de ahi que las *P* variables son ponderadas y sumadas para obtener una nueva variable que le llamamos Componente Principal)

Esta es una guía para el usuario que pretende hacer uso del **Análisis de Componentes Principales**. Como guía, no pretende ser un análisis exhaustivo en la teoria que subyace detrás del instrumento. 

Objetivos 

>1. Identificar la interacción de las variables en la dinamica del sistema
>
>2. Reducir la dimensionalidad de los datos
>
>3. Reducir la redundancia en los datos
>
>4. Filtrar parte del ruído presente en los datos
>
>5. Comprimir los datos

### Ventajas {-}

1. Preserva la estructura global de las observaciones

2. Altamente eficiente para conjuntos de datos grandes

3. Puede evidenciar la importancia de algunas variables en el conjunto entero

### Desventajas {-}

1. Puede sufrir con problemas de escala

2. Sensible a datos extremos

3. Puede no ser inmediatamente intuititivo

### Proceso 

No obstante -y a riesgo de perder absoluta formalidad- podríamos sobre simplificar el proceso describiendolo en unos pocos pasos (cada uno de ellos conteniendo una cantidad importante de conocimiento)

* 1 - Centrar los datos con respecto a su promedio
* 2 - Computar la Matriz de Varianzas y Covarianzas [*MVC*]
* 3 - Obtener los Valores $\lambda=[\lambda_{1},\lambda_{2}, ..., \lambda_{p}]$ y Vectores Propios    $V=[v_{1}, v_{2}, ..., v_{p}]$ de *MCV*
* 4 - Seleccionar los *K* Valores Propios que más aportan a explicar la varianza tal que $\text {APORTE a la VARIANZA}= \frac{v_{1} + v_{2} + ... + v_{k}}{\sum_{i=1}^{p} v_{i}}$
* 5 - Proyectar la matriz de datos X en el subespacio de las variables, para los k-componentes seleccionados $X_{Transformado = V_{K}^t X}$ (Como nota, basta con premultiplicar a los datos transformados por $V_{K}$ para recuperar los valores originales de X $V_{K}X_{Transformado = V_{K}V_{K}^t X}$=X ... Eso es sexi!)

### Herramientas Visuales {.tabset}

Dos gráficas, son ampliamente empleadas para determinar la cantidad de *componentes* que el analista define como relevantes o al menos aceptable.
Los valores propios obtenidos del ACP están directamente relacionados con la varianza que represenar, tanto, que la suma de los valores propios $\sum_{i=1}^{p}\lambda_{i}=p$ ... Y esto es grandioso!

#### Screeplot

El Screeplot muestra el valor del valor propio (asociado al vector propio). Su interpretación inmediata se relaciona con el valor = 1. Cada valor propio con valor superior a dicho valor, nos indica un aporte de varianza mayor que el observado en los datos sin transformar.

#### Porción Acumulada de Varianza

El segundo elemento describe la porcion de varianza acumulada por el componente principal en cuestión, esto a partir de:

$$Porción~Acumulada~de~Varianza ~=~ \frac{\sigma_i^2}{\sum_{i=1}^N \sigma_i^2}$$

###  Conclusiones
PCA es una excelente opcion en el proceso de analisis exploratorio de los datos. Uno de sus principales objetivos, es filtrar el ruido presente en los datos, para poder quedarse con la melodia de ellos.

## EJEMPLO ACP: USA Arrest{.tabset}
Continuaremos con los datos de USArrest (para dar continuidad a nuestra clase de k-Medias)

Lo primero que haremos es limpiar la memoria 
```{r}
  rm(list=ls())
```
### CONJUNTO DE DATOS

```{r,warning=FALSE}
  library(datarium)
  df <- USArrests
  DT::datatable(df,                    
                rownames = TRUE, 
                filter="top",
                options = list(pageLength = 15, scrollX=T)
                )
```


### ESTADISTICAS DESCRIPTIVAS

```{r}
  df.resumen=summary(df)
  DT::datatable(df.resumen,                    
                rownames = TRUE, 
                filter="top",
                options = list(pageLength = 15, scrollX=T)
                )
```


### MATRIZ VISUAL

Iniciamos visualizando las relaciones -entre pares- de datos CUANTITATIVOS. Es importante anotar que las columnas 1 y 8 de este conjunto están ocupadas por variles con TEXTO por lo que deben ser excluídas del análisis cuantitativo, esto *R* lo maneja de manera magnífica.Basta con excluir del set de datos las variables que causan el problema, esto lo hacemos agregando un negativo a la columna que queremos excluir. 

Pero además, notar que *R* es capaz de excluir varias a la vez, para ello se lo indicamos en un vector, que le indica opere con estas variables, para ello introducimos c()


```{r, out.width = "800px"}
pairs(df)
```

El conjunto de datos, sera separando en dos grupos variables las ACTIVAS y las ILUSTRATIVAS
```{r}

```

### TABLA (n,p)
Procedemos a contar la cantidad de registros [*N*], así como la cantidad de dimensiones [*P*]
Cantidad de Individuos
```{r}
  n <- nrow(df)
  p <- ncol(df)
  
  R=paste("Cantidad de Individuos: [", n, "] Cantidad de Variables: [",p,"]")
  print(R)
```

### VARCOV {.tabset}

#### **NO** ESCALADOS
Covarianzas:
```{r}
  cov(df)
```

Correlaciones:
```{r}
  cor(df)
```

#### **ESCALADOS**
Notar que el escalamiento sólo afecta a la matriz de COVARIANZAS no así a la de Correlaciones, esto pues no hemos modificado la estructura de los datos con el escalamiento

Covarianzas:
```{r}
  cov(scale(df))
```

Correlaciones:
```{r}
  cor(scale(df))
```

## ACP a *pata* {.tabset}

#### NO ESCALADO
```{r, out.width = "600px"}
S <- cov(df)
sum(diag(S))
s.eigen <- eigen(S)
s.eigen$values
B=0
for (s in s.eigen$values) {
                          A=s / sum(s.eigen$values) 
                          B=A+B
                          C=data.frame(cbind(A,s))
                          print(C)
                          }
plot(s.eigen$values, 
     xlab = 'Valor Propio (n)', 
     ylab = 'Valor Propio (q)', 
     main ='Scree')
lines(s.eigen$values)

s.eigen$vectors

```

#### ESCALADO
```{r, out.width = "600px"}
S <- cov(scale(df))
sum(diag(S))
s.eigen <- eigen(S)
s.eigen$values
B=0
for (s in s.eigen$values) {
                          A=s / sum(s.eigen$values) 
                          B=A+B
                          C=data.frame(cbind(A,s))
                          print(C)
                          }
plot(s.eigen$values, 
     xlab = 'Valor Propio (n)', 
     ylab = 'Valor Propio (q)', 
     main ='Scree')
lines(s.eigen$values)

s.eigen$vectors

PC <- as.matrix(scale(df)) %*% s.eigen$vectors
plot(PC[,1],PC[,2])

#biplot(prcomp(df))

```

## ACP con *FactoMineR* {.tabset}

Con FactoMineR el camino es más corto

```{r, out.width = "600px",include=FALSE}
  acp.df.FM=FactoMineR::PCA(df)
```

ScreePlot
```{r, out.width = "800px"}
  fviz_eig(acp.df.FM, addlabels = TRUE, ylim = c(0, 80))
```

A fin de que la visualización sea amigable, vale la pena agregar un poco de color y cambiar el output que la herramienta entrega por defecto


- VALORES PROPIOS Y APORTE DE VARIANZA
```{r}
  print(acp.df.FM$eig)
```

### RES: VARIABLES

```{r, out.width = "800px"}
  factoextra::fviz_pca_var(
                          acp.df.FM, col.var="contrib",
                          gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
                          repel = TRUE # Avoid text overlapping
                          )
```

-Coordenadas
```{r}
  print(acp.df.FM$var$coord)
```

-$\text {Coseno}^2$
```{r}
  print(acp.df.FM$var$cos2)
```

-Contribucion
```{r}
  print(acp.df.FM$var$contrib)
```

-Aporte a la dimensión
```{r, out.width = "600px"}
  corrplot(acp.df.FM$var$contrib, is.corr=FALSE)
```


### RES: INDIVIDUOS

```{r}
fviz_pca_ind(
             acp.df.FM,
             col.ind = "cos2", # Color by the quality of representation
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )
```   

-Coordenadas
```{r}
  print(acp.df.FM$ind$coord)
```

-$\text {Coseno}^2$
```{r}
  print(acp.df.FM$ind$cos2)
```

-Contribución
```{r}
  print(acp.df.FM$ind$contrib)
```

### RES: AMBOS     

```{r}
fviz_pca_biplot(acp.df.FM, repel = TRUE,
                col.var = "#2E9FDF", # Variables color
                col.ind = "#696969"  # Individuals color
                )
```
